{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/home/aew492/CFE_gradients/code')\n",
    "from generate_mock_list import MockSet\n",
    "import load_tools\n",
    "import globals\n",
    "globals.initialize_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = globals.data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to write a function that takes in survey parameters and returns the expected gradient recovery.\n",
    "\n",
    "- $V \\rightarrow \\sigma$ given fixed tracer density $n$, where $\\sigma$ is the precision of recovery\n",
    "- $V, \\gamma \\rightarrow \\gamma = 0 \\pm \\mathrm{XX}\\sigma$, where XX$\\sigma$ is the confidence that $\\gamma = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling of $\\sigma_\\gamma$ with volume $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simulated results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $m$ determines relative gradient, we want to match the ratio $m/L$ so that all the mocks here have the same physical gradient:\n",
    "$$\n",
    "\\frac{m}{L} = .001\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For now, use what we have, 2D same_omega mocks, but REPLACE THESE with 3D random_omegas mocks to be more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = [500, 750, 1000]\n",
    "ms = [0.5, 0.75, 1.0]\n",
    "n = '2e-4'\n",
    "nmocks = 1000\n",
    "grad_dim = 2\n",
    "b = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovered gradient amplitudes\n",
    "grads_rec = np.empty((len(Ls), nmocks, 3))\n",
    "for i, L in enumerate(Ls):\n",
    "    mockset = MockSet(L, n, rlzs=nmocks)  # initialize mock set\n",
    "    mockset.add_gradient(grad_dim, m=ms[i], b=b)  # add gradient\n",
    "    amps_dict = load_tools.check_grad_amps(mockset, method='suave', plot=False, return_amps=True)\n",
    "    grads_rec[i] = amps_dict['grads_rec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance in recovered gradient, by component\n",
    "grads_rec_std = np.std(grads_rec, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_rec_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (<ipython-input-40-702c6bd24d30>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-702c6bd24d30>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    ax.set_ylim((1e-4, 1e-(2.5))\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "labels = ['x', 'y', 'z']\n",
    "\n",
    "for i, grad_std in enumerate(grads_rec_std.T):\n",
    "    ax.plot(Ls, grad_std, marker='.', lw=1, label=labels[i])\n",
    "\n",
    "ax.axhline(0, color='k', lw=0.5, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim((1e-4, 1e-2.5\n",
    "ax.set_xlabel(r'L ($h^{-1}$Mpc)')\n",
    "ax.set_ylabel(r'$\\sigma_{\\gamma, x}$')\n",
    "ax.set_title(r'Std. in $\\gamma_x$ as a function of boxsize $L$')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roman's equation\n",
    "# ** predicted variance in the 2pcf at a certain length scale r\n",
    "def xi_err_pred(r, V, n, k, Pk, nk=1000):\n",
    "\n",
    "    # interpolate between the discrete (k, Pk) values to get a finer scale for integration\n",
    "    kfine = np.linspace(min(k), max(k), nk)\n",
    "    Pkfine = np.interp(kfine, k, Pk)\n",
    "    \n",
    "    # multiplicative constant\n",
    "    const = 1 / (2*np.pi*V)\n",
    "\n",
    "    # function of k that we want to integrate\n",
    "    def k_func(k, Pk):\n",
    "        return (k/r) * (Pk+(1/n))**2 * (special.jv(1/2, k*r))**2\n",
    "\n",
    "    # construct our array, and integrate using trapezoid rule\n",
    "    k_func_arr = np.array([k_func(k, Pkfine[i]) for i, k in enumerate(kfine)])\n",
    "    trapz = integrate.trapz(k_func_arr, x=kfine)\n",
    "\n",
    "    return const*trapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suaveenv",
   "language": "python",
   "name": "suaveenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
